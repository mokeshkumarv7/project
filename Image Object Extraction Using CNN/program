# =========================
# IMPORTS
# =========================
from google.colab import files
import torch
import torchvision
import numpy as np
import cv2
import matplotlib.pyplot as plt
from PIL import Image

# =========================
# UPLOAD IMAGE (COLAB SAFE)
# =========================
print("Upload the image:")
uploaded = files.upload()
image_path = list(uploaded.keys())[0]

image_pil = Image.open(image_path).convert("RGB")
image_np = np.array(image_pil)
h, w, _ = image_np.shape

# =========================
# LOAD PRETRAINED DEEPLABV3
# =========================
model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True)
model.eval()

# =========================
# PREPROCESS IMAGE
# =========================
transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

input_tensor = transform(image_pil).unsqueeze(0)

# =========================
# SEMANTIC SEGMENTATION
# =========================
with torch.no_grad():
    output = model(input_tensor)["out"][0]

labels = output.argmax(0).byte().cpu().numpy()

# COCO class 15 = PERSON
semantic_mask = (labels == 15).astype(np.uint8)

# Resize (safety)
semantic_mask = cv2.resize(
    semantic_mask, (w, h), interpolation=cv2.INTER_NEAREST
)

# =========================
# GRABCUT REFINEMENT
# =========================
mask = np.zeros((h, w), np.uint8)
mask[semantic_mask == 1] = cv2.GC_PR_FGD
mask[semantic_mask == 0] = cv2.GC_BGD

bgdModel = np.zeros((1, 65), np.float64)
fgdModel = np.zeros((1, 65), np.float64)

cv2.grabCut(
    image_np, mask, None, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_MASK
)

final_mask = np.where(
    (mask == cv2.GC_FGD) | (mask == cv2.GC_PR_FGD), 1, 0
).astype(np.uint8)

# =========================
# METRICS (NO GROUND TRUTH)
# =========================
foreground_coverage = np.sum(final_mask) / final_mask.size * 100

edges_img = cv2.Canny(
    cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY), 100, 200
)
edges_mask = cv2.Canny(final_mask * 255, 100, 200)

edge_alignment = np.sum(
    (edges_img > 0) & (edges_mask > 0)
) / (np.sum(edges_mask > 0) + 1e-8)

mask_change = np.sum(semantic_mask != final_mask) / final_mask.size * 100

# =========================
# DISPLAY RESULTS
# =========================
plt.figure(figsize=(15,5))

plt.subplot(1,3,1)
plt.imshow(image_np)
plt.title("Original Image")
plt.axis("off")

plt.subplot(1,3,2)
plt.imshow(final_mask, cmap="gray")
plt.title("Final Mask")
plt.axis("off")

plt.subplot(1,3,3)
plt.imshow(image_np * final_mask[:,:,None])
plt.title("Extracted Object")
plt.axis("off")

plt.show()

# =========================
# PRINT METRICS
# =========================
print("========== SEGMENTATION QUALITY METRICS ==========")
print(f"Foreground Coverage (%)        : {foreground_coverage:.2f}")
print(f"Edge Alignment Score (0â€“1)     : {edge_alignment:.4f}")
print(f"Mask Change After Refinement % : {mask_change:.2f}")
